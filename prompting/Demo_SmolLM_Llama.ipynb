{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c9792f-6750-44cb-8383-25f39db20c32",
   "metadata": {},
   "source": [
    "# Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7595eec-b8be-4216-b84b-3183b45b19e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Installations, imports\n",
    "\n",
    "!pip install langchain --quiet\n",
    "!pip install langchain_community --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install accelerate --quiet\n",
    "!pip install nltk --quiet\n",
    "\n",
    "\n",
    "# Required Libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Find the best available device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # Use GPU if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9142bd5-5057-4adb-9def-43d778543338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import et traitement data Paleosaurus\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Paleosaurus_ISTEX.csv')\n",
    "\n",
    "text_contents = []\n",
    "for idx,row in df.iterrows():\n",
    "    text_contents.append(f\"Title: {row['title']}\\nAbstract: {row['abstract']}\\n\")\n",
    "\n",
    "#print ([text_contents[a] for a in range(10)])\n",
    "\n",
    "htfl = pd.DataFrame(text_contents, columns=['text'])\n",
    "htfl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006f660-f002-478e-913a-478a5fd12083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13797386-caac-43d6-8958-32295bb9050a",
   "metadata": {},
   "source": [
    "# SmoLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a7dcf1-2cf8-45d9-b799-aa2b376a7dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_identifier = 'HuggingFaceTB/SmolLM2-1.7B-Instruct'\n",
    "llm = pipeline(model=model_identifier, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ae4856-9fb3-4871-a075-cfd3c5ca7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition de prompt pour SmolLM\n",
    "def SmolLM_chat_def(doc) :\n",
    "    chat = [\n",
    "                    {\n",
    "                        'role':'system',\n",
    "                        'content':'''As an excellent automatic term extraction (ATE) system, extract the terms in the Paleoclimatology domain given the following text delimited. Named entities are not considered as terms.\n",
    "                        Separate terms with commas. Ensure each term is from the Paleoclimatology domain, each term represents a main topic from the document, and provide no additional information.\n",
    "                        Make sure you only return the terms and say nothing else. For example, dont say: \"Sure, Id be happy to help! Based on the information provided in the document\".\n",
    "    \n",
    "                        Output Format: [list of terms present]\n",
    "                        If no terms are presented, keep it empty list: [].'''\n",
    "                    },\n",
    "                    {'role':'user', 'content': 'The Hydroclimate and Environmental Response to Middle Miocene Warming in the Southwestern USA: Stable Isotope Evidence .'},\n",
    "                    {'role':'assistant', 'content': '[Hydroclimate, Environmental Response, Middle Miocene, Warming, Stable Isotope Evidence]'},\n",
    "                    {'role':'user', 'content': 'Late Pleistocene Sediment Provenance and Paleoenvironmental Changes in the East Siberian Shelf Margin: Insights From Mineralogical and Nd Isotope Analysis .'},\n",
    "                    {'role':'assistant', 'content': '[Late Pleistocene, Sediment Provenance, Paleoenvironmental Changes, East Siberian Shelf Margin, Mineralogical Analysis, Nd Isotope Analysis]'},\n",
    "                    {'role':'user', 'content': 'Moreover , there is yet to be established a common consensus being used in current assays .'},\n",
    "                    {'role':'assistant', 'content': '[]'},\n",
    "                    {'role':'user', 'content': doc}\n",
    "                ]\n",
    "    return chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34606a94-660b-4eda-ad91-a934d7e3687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K-Ar dating, Age, King George Island, West Antarctica, Stratigraphy, Early Eocene, Arctowski Interglacial, Paleocene, Oligocene-Miocene Boundary, Late Cretaceous, Paleocene, Late Eocene-Early Oligocene, Early Miocene, Wawel Interglacial, Legru Glaciation, Polonez Glaciation, Wesele Interglacial, Early Miocene glaciation, Late Cretaceous, Cretaceous]\n",
      "CPU times: user 4 s, sys: 124 ms, total: 4.13 s\n",
      "Wall time: 4.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chat = SmolLM_chat_def(htfl[\"text\"][1])\n",
    "\n",
    "prompt = llm.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "# Generate prediction\n",
    "generation = llm(prompt, max_new_tokens = 2000)\n",
    "\n",
    "new_text = generation[0]['generated_text'][len(prompt):]\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95dadb-72f5-4374-addb-a567d80d247b",
   "metadata": {},
   "source": [
    "# Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e33c0-7d21-4492-a570-f025febdca1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.23s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Chargement modèle et préparation\n",
    "from huggingface_hub import login\n",
    "\n",
    "token = \"your-token-here\"\n",
    "\n",
    "login(token=token)\n",
    "\n",
    "# Define model and tokenizer\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens = 256,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ") #max_length=2000,\n",
    "\n",
    "llm2 = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': 0})\n",
    "\n",
    "#Definition extracteur des résultats\n",
    "def extract_after_last_inst(text):\n",
    "    # This splits after the last instruction and extracts only the final output\n",
    "    segments = text.split(\"[/INST]\")\n",
    "    if len(segments) > 1:\n",
    "        return segments[-1].strip().replace(\"</s>\", \"\").strip()\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5660866-2101-426a-a703-53f3cb131d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "                <s>\n",
    "                [INST]\n",
    "                <<SYS>>\n",
    "                As an excellent automatic term extraction (ATE) system, extract the terms in the Paleoclimatology domain given the following text delimited. Named entities are not considered as terms.\n",
    "                    Separate terms with commas. Ensure each term is from the Paleoclimatology domain, each term represents a main topic from the document, and provide no additional information.\n",
    "                    Make sure you only return the terms and say nothing else. For example, dont say: \"Sure, Id be happy to help! Based on the information provided in the document\".\n",
    "\n",
    "                    Output Format: [list of terms present]\n",
    "                    If no terms are presented, keep it empty list: [].\n",
    "\n",
    "\n",
    "                Examples of the output format:\n",
    "                <</SYS>>\n",
    "                Sentence: ```The Hydroclimate and Environmental Response to Middle Miocene Warming in the Southwestern USA: Stable Isotope Evidence .```\n",
    "                Domain: Paleoclimatology\n",
    "                [/INST]\n",
    "                Output: \"[Hydroclimate, Environmental Response, Middle Miocene, Warming, Stable Isotope Evidence]\"\n",
    "                </s>\n",
    "\n",
    "                <s>\n",
    "                [INST]\n",
    "                Sentence: ```Late Pleistocene Sediment Provenance and Paleoenvironmental Changes in the East Siberian Shelf Margin: Insights From Mineralogical and Nd Isotope Analysis  .```\n",
    "                Domain: Paleoclimatology\n",
    "                [/INST]\n",
    "                Output: \"[Late Pleistocene, Sediment Provenance, Paleoenvironmental Changes, East Siberian Shelf Margin, Mineralogical Analysis, Nd Isotope Analysis]\"\n",
    "                </s>\n",
    "\n",
    "                <s>\n",
    "                [INST]\n",
    "                Sentence: ```Moreover , there is yet to be established a common consensus being used in current assays .```\n",
    "                Domain: Paleoclimatology\n",
    "                [/INST]\n",
    "                Output: \"[]\"\n",
    "                </s>\n",
    "\n",
    "                <s>\n",
    "                [INST]\n",
    "                Sentence: ```{text}```\n",
    "                Domain: Paleoclimatology\n",
    "                [/INST]\n",
    "                \"\"\"\n",
    "\n",
    "# Create the LangChain prompt and chain\n",
    "template = PROMPT\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a774e132-4636-4929-97ff-fcabc59437f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [\"Late Cretaceous\", \"early Tertary\", \"King George Island\", \"West Antarctica\", \"stratigraphic distribution\", \"palaeoclimatic significance\", \"K-Ar dating\", \"Late Cretaceous\", \"Paleocene\", \"Eocene\", \"Kraków Glaciation\", \"Arctowski Interglacial\", \"Oligocene\", \"Miocene\", \"Wawel Interglacial\", \"Polonez Glaciation\", \"Wesele Interglacial\", \"Legru Glaciation\", \"Melville Glaciation\"]\n",
      "CPU times: user 3min 46s, sys: 3.38 s, total: 3min 49s\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Execution\n",
    "\n",
    "output_raw = llm_chain.run({\"text\": htfl[\"text\"][1]})\n",
    "output_clean = extract_after_last_inst(output_raw)\n",
    "\n",
    "print(output_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a10a0-cb7b-48d9-9904-108f27576eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
